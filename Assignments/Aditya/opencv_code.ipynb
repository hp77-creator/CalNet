{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCpitcOIoU7n"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "image = cv2.imread('/content/dave-goudreau-e-Jq0_SQux8-unsplash.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBwkM2wgo5w8"
      },
      "source": [
        "TRANSLATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh7XqFtHorKN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "3d789b19-2e07-445f-9274-a31932a715b5"
      },
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "image = cv2.imread('/content/dave-goudreau-e-Jq0_SQux8-unsplash.jpg')#storing width and height\n",
        "height,width = image.shape[:2]\n",
        "\n",
        "quarter_height, quarter_width = height/4, width/4\n",
        "\n",
        "#translation matrix\n",
        "T = np.float32([[1, 0, quarter_width], [0, 1, quarter_height]])\n",
        "\n",
        "#warpAffine to translate image\n",
        "img_translation = cv2.warpAffine(image, T, (width, height))\n",
        "cv2.imshow('translation', img_translation)\n",
        "\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-aa4a8d1edd37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dave-goudreau-e-Jq0_SQux8-unsplash.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#storing width and height\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mquarter_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquarter_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTKh-tISq7HH"
      },
      "source": [
        "ROTATIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGCC9Srzq-EX"
      },
      "source": [
        "#rotation matrix = getRotationMatrix2d(coordinates, angle, scaling factor)\n",
        "rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), 90, 1)\n",
        "\n",
        "#warpAffine to rotate image\n",
        "img_rotation = cv2.warpAffine(image, rotation_matrix, (height, width))\n",
        "cv2.imshow(' rotation', img_rotation)\n",
        "\n",
        "#another method to rotate image by 90 degrees\n",
        "img = cv2.transpose(image)\n",
        "cv2.imshow(' rotation', img)\n",
        "\n",
        "cv2.waitKey()\n",
        "cv2.destroyAllWindows\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf_BupeEsuTe"
      },
      "source": [
        "SCALING, RESIZING, INTERPOLATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAHVCFdZsyZW"
      },
      "source": [
        "#cv2.INTER_AREA: This is used when we need to shrink an image.\n",
        "#cv2.INTER_CUBIC: This is slow but more efficient.\n",
        "c#v2.INTER_LINEAR: This is primarily used when zooming is required. This is the default interpolation technique in OpenCV. \n",
        "half = cv2.resize(image, (0, 0), fx = 0.1, fy = 0.1)\n",
        "bigger = cv2.resize(image, (1050, 1610))\n",
        " \n",
        "stretch_near = cv2.resize(image, (780, 540),\n",
        "               interpolation = cv2.INTER_NEAREST)\n",
        " \n",
        " \n",
        "Titles =[\"Original\", \"Half\", \"Bigger\", \"Interpolation Nearest\"]\n",
        "images =[image, half, bigger, stretch_near]\n",
        "count = 4\n",
        " \n",
        "for i in range(count):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.title(Titles[i])\n",
        "    plt.imshow(images[i])\n",
        " \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoaNTLKZSwWd"
      },
      "source": [
        "CROPPING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxjiIKBWSyQu"
      },
      "source": [
        "# Cropping an image\n",
        "cropped_image = img[80:280, 150:330]\n",
        "\n",
        "# Display cropped image\n",
        "cv2.imshow(\"cropped\", cropped_image)\n",
        "\n",
        "# Save the cropped image\n",
        "cv2.imwrite(\"Cropped Image.jpg\", cropped_image)\n",
        "\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhPAnWZ5S46c"
      },
      "source": [
        "BITWISE OPERATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTaTnDOuTOdQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "84195f85-84c2-4c2d-eee9-4e79e387bd2f"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "image = cv2.imread('/content/dave-goudreau-e-Jq0_SQux8-unsplash.jpg')\n",
        "#75 is used to add brightness to our image, more the number, more the brightness\n",
        "M = np.ones(image.shape, dtype = \"uint8\")*75\n",
        "\n",
        "added = cv2.add(M, image)\n",
        "cv2.imshow('added', added)\n",
        "\n",
        "subtracted = cv2.subtract(M, image)\n",
        "cv2.imshow('subtracted', subtracted)\n",
        "\n",
        "cv2.waitKey()\n",
        "cv2.destroyAllWindows\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1b620963789f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dave-goudreau-e-Jq0_SQux8-unsplash.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#75 is used to add brightness to our image, more the number, more the brightness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0madded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1svba7mUN4M"
      },
      "source": [
        "CREATING SOME SIMPLE IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO-bW0bKUnZf"
      },
      "source": [
        "#making a square\n",
        "square = np.zeros((300, 300), np.uint8)\n",
        "cv2.rectangle(square, (50, 50), (250, 250), 255, -2)\n",
        "cv2.imshow('square', square)\n",
        "cv2.waitkey()\n",
        "\n",
        "#making an ellipse\n",
        "ellipse = np.zeros((300, 300), np.uint8)\n",
        "cv2.ellipse(ellipse, (150, 150), (150, 150), 30, 0, 180, 255, -1)\n",
        "cv2.imshow('ellipse', ellipse)\n",
        "cv2.waitkey()\n",
        "\n",
        "#to show where the intersect\n",
        "And = cv2.bitwise_and(square, ellipse)\n",
        "cv2.imshow('And', And)\n",
        "cv2.waitkey()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRcUrCw8V99w"
      },
      "source": [
        "BLURRING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A5xhBIoV9jx"
      },
      "source": [
        "#creating a 3x3 kernel\n",
        "kernel_3x3 = np.ones((3,3), np.float32)/9\n",
        "\n",
        "#we use cv2.filter2D for blurring out our image\n",
        "blurred1 = cv2.filter2D(image, -1, kernel_3x3)\n",
        "imshow('blurred1', blurred1)\n",
        "cv2.waitkey()\n",
        "\n",
        "#we can also convolve it with 7x7 matrix\n",
        "kernel_7x7 = np.ones((7,7), np.float32)/49\n",
        "blurred2 = cv2.filter2D(image, -1, kernel_7x7)\n",
        "imshow('blurred2', blurred2)\n",
        "cv2.waitkey()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or9pXtMjYVia"
      },
      "source": [
        "OTHER BLURRING METHODS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HZte59hYYNq"
      },
      "source": [
        "#averaging done with normalised box filter\n",
        "#this takesthe pixels under the box and relaces the central element\n",
        "#box needs to be odd and positive\n",
        "blur = cv2.blur(image, (3, 3))\n",
        "cv2.imshow('Averaging', blur)\n",
        "\n",
        "#insted of box filter, use gaussian kernel\n",
        "gaussian = cv2.GaussianBlur(image, (7,7), 0)\n",
        "cv2.imshow('Gaussian blurring', gaussian)\n",
        "cv2.waitkey(0)\n",
        "\n",
        "#takes median of all pixels of the kernel\n",
        "# replaces central element with median\n",
        "median = cv2.medianBlur(image, 5)\n",
        "cv2.imshow('Median blur', median)\n",
        "cv2.waitkey(0)\n",
        "\n",
        "#bilateral is useful while removing noise and keeping edges sharp\n",
        "bilateral = cv2.bilateralFilter(image, 9, 75, 75)\n",
        "cv2.imshow('bilateral blur', bilateral)\n",
        "cv2.waitkey(0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb0ODCv-aDaI"
      },
      "source": [
        "IMAGE DE-NOISING NON LOCAL MEANS DENOISING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-ImPo9_aL_n"
      },
      "source": [
        "dst = cv2.fastNlMeansDenoisingColored(image, None, 6, 6, 7, 21)\n",
        "\n",
        "cv2.imshow('Fast Means Denoising', dst)\n",
        "cv2.waitkey(0)\n",
        "\n",
        "#there are other ways of non local means denoising\n",
        "# cv2.fastNIMeansDenoising()- works with single grayscale images\n",
        "# cv2.fastNiMeansDenoising()- works with colored image\n",
        "# cv2.fastNIMeansDenoisingMulti()- works with image sequence captured in short period of time\n",
        "# cv2.fastNIMeansDenoisingColoredMulti()- same as above but for colored image (above is for grayscale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1ssqjzZbUdW"
      },
      "source": [
        "SHARPENING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQAjDpOvbWCV"
      },
      "source": [
        "#create sharpening kernel\n",
        "kernel_sharpening = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
        "\n",
        "#applying different kernels to sharpened image\n",
        "sharpened = cv2.filter2D(image, -1, kernel_sharpening)\n",
        "\n",
        "cv2.imshow('sharpened', sharpened)\n",
        "cv2.waitkey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGtnfmwgmLNJ"
      },
      "source": [
        "THRESHHOLDING AND BINARIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDC6hUR4mReU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "ff089493-8368-4498-9c22-5c607d2d9718"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "image = cv2.imread('/content/dave-goudreau-e-Jq0_SQux8-unsplash.jpg')\n",
        "ret,thresh1 = cv2.threshold(image,127,255,cv2.THRESH_BINARY)\n",
        "ret,thresh2 = cv2.threshold(image,127,255,cv2.THRESH_BINARY_INV)\n",
        "ret,thresh3 = cv2.threshold(image,127,255,cv2.THRESH_TRUNC)\n",
        "ret,thresh4 = cv2.threshold(image,127,255,cv2.THRESH_TOZERO)\n",
        "ret,thresh5 = cv2.threshold(image,127,255,cv2.THRESH_TOZERO_INV)\n",
        "cv2.imshow(thresh1)\n",
        "cv2.waitkey(0)\n",
        "cv2.clearAllWIndows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DisabledFunctionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6cce9576e717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresh4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_TOZERO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthresh5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_TOZERO_INV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclearAllWIndows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_snippet",
                "actionText": "Search Snippets for cv2.imshow",
                "snippetFilter": "cv2.imshow"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-QAKmuQmvDL"
      },
      "source": [
        "ADAPTIVE THRESHHOLDING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYllDdf8muxz"
      },
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "img = cv.imread('noisy2.png',0)\n",
        "# global thresholding\n",
        "ret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
        "# Otsu's thresholding\n",
        "ret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "# Otsu's thresholding after Gaussian filtering\n",
        "blur = cv.GaussianBlur(img,(5,5),0)\n",
        "ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "# plot all the images and their histograms\n",
        "images = [img, 0, th1,\n",
        "          img, 0, th2,\n",
        "          blur, 0, th3]\n",
        "titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n",
        "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
        "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
        "for i in xrange(3):\n",
        "    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n",
        "    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n",
        "    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n",
        "    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n",
        "    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n",
        "    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfM3qOq0nfg_"
      },
      "source": [
        "EROSION AND DILATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2AIrOeGne2W"
      },
      "source": [
        "import cv2  \n",
        "import numpy as np  \n",
        "img = cv2.imread(r'C:\\Users\\DEVANSH SHARMA\\baloon.jpg', 1)  \n",
        "# cv2.erode(src, dst, kernel) cv2.dilate(src, dst, kernel)  \n",
        "\n",
        "kernel = np.ones((5,5), np.uint8)  \n",
        "img_erosion = cv2.erode(img, kernel, iterations=1)  \n",
        "img_dilation = cv2.dilate(img, kernel, iterations=1)  \n",
        "cv2.imshow('Input', img)  \n",
        "cv2.imshow('Erosion', img_erosion)  \n",
        "cv2.waitKey(0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmM-d_vNopT1"
      },
      "source": [
        "EDGE DETECTION AND IMAGE GRADIENTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwBKMSYEo6HD"
      },
      "source": [
        "height, width =image.shape\n",
        "\n",
        "#extract sobel edges\n",
        "sobel_x = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
        "sobel_y = cv2.Sobel(image, cv2.CV_64F, 1 , 0, ksize=5)\n",
        "sobel_OR = cv2.bitwise_or(sobel_x, sobel_y)\n",
        "laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
        "\n",
        "#show everything\n",
        "cv2.imshow('sobel_x', sobel_x)\n",
        "cv2.waitkey(0)\n",
        "cv2.imshow('sobel_y', sobel_y)\n",
        "cv2.waitkey(0)\n",
        "cv2.imshow('sobel_OR', sobel_OR)\n",
        "cv2.waitkey(0)\n",
        "cv2.imshow('laplacian', laplacian)\n",
        "cv2.waitkey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhLuqQezqUTT"
      },
      "source": [
        "GET PERSPECTIVE TRANSFORM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niFP-E1Xcvq2"
      },
      "source": [
        "# coordinates of 4 corners of image\n",
        "points_A = np.float32([320,15], [700,215], [85, 610], [530, 780])\n",
        "\n",
        "#coordinates of desired paper (here a4)\n",
        "points_B = np.float32([0,0], [420,0], [0,594], [594,420])\n",
        "\n",
        "#use the 2 sets of 4 pointe to compute the perspective transformation of matrix, M\n",
        "M = cv2.getPerspectiveTransform(points_A, points_B)\n",
        "\n",
        "warped = cv2.warpPerspective(image, M, (420, 594))\n",
        "\n",
        "cv2.imshow('warPerspective', warped)\n",
        "waitkey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GEvePtdeT4Q"
      },
      "source": [
        "AFFINE TRANSFORM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uEfxIBpeWqD"
      },
      "source": [
        "# coordinates of 3 corners of image\n",
        "points_A = np.float32([320,15], [700,215], [85, 610])\n",
        "\n",
        "#coordinates of desired paper (here a4)\n",
        "points_B = np.float32([0,0], [420,0], [0,594], [594,420])\n",
        "\n",
        "M = cv2.getAffineTransform(points_A, points_B)\n",
        "\n",
        "warped = cv2.warpAffine(image, M, (420, 594))\n",
        "\n",
        "cv2.imshow('warPerspective', warped)\n",
        "waitkey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RP3IiE9Uibc"
      },
      "source": [
        "CONTOURS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJrQGOrCUqMY"
      },
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "im = cv.imread('test.jpg')\n",
        "imgray = cv.cvtColor(im, cv.COLOR_BGR2GRAY)\n",
        "ret, thresh = cv.threshold(imgray, 127, 255, 0)\n",
        "im2, contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# draw contours\n",
        "\n",
        "#To draw all the contours in an image:\n",
        "cv.drawContours(img, contours, -1, (0,255,0), 3)\n",
        "\n",
        "#To draw an individual contour, say 4th contour:\n",
        "cv.drawContours(img, contours, 3, (0,255,0), 3)\n",
        "\n",
        "#But most of the time, below method will be useful:\n",
        "cnt = contours[4]\n",
        "cv.drawContours(img, [cnt], 0, (0,255,0), 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6aqCUaycQx3"
      },
      "source": [
        "SORTING CONTOURS BY AREA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXAguk2ZcU2E"
      },
      "source": [
        "mport cv2\n",
        "\n",
        "image= cv2.imread('Shapes.png')\n",
        "original_image= image\n",
        "\n",
        "gray= cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "edged= cv2.Canny(gray, 50,200)\n",
        "\n",
        "contours, hierarchy= cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "\n",
        "cv2.imshow('Original Image', image)\n",
        "cv2.waitKey(0)\n",
        "\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "def get_contour_areas(contours):\n",
        "\n",
        "    all_areas= []\n",
        "\n",
        "    for cnt in contours:\n",
        "        area= cv2.contourArea(cnt)\n",
        "        all_areas.append(area)\n",
        "\n",
        "    return all_areas\n",
        "\n",
        "\n",
        "\n",
        "print (\"Contour Areas before Sorting\", get_contour_areas(contours))\n",
        "\n",
        "sorted_contours= sorted(contours, key=cv2.contourArea, reverse= True)\n",
        "\n",
        "print (\"Contour Areas after Sorting\", get_contour_areas(sorted_contours))\n",
        "\n",
        "\n",
        "for c in sorted_contours:\n",
        "    cv2.drawContours(original_image, [c], -1, (255,0,0),10)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.imshow('Contours By Area', original_image)\n",
        "\n",
        "\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2J--33ScV3z"
      },
      "source": [
        "SORTING CONTOURS BY POSITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhfEMrHpcZxi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_w3pFVGeEjN"
      },
      "source": [
        "APPROXIMATING CONTOURS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqYqrR7-eK3I"
      },
      "source": [
        "orig_image = image.copy()\n",
        "\n",
        "#grayscale and binarize\n",
        "gray = cv2.cvtColor(image, cv2.COLOUR_BGR2GRAY)\n",
        "ret, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "#find contours\n",
        "contours, heirarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "#iterate through each contour and compute the bounding rectangle\n",
        "for c in contours:\n",
        "  #calculate accuracy as percent of contour perimeter\n",
        "  accuracy = 0.03*cv2.arcLength(c, True)\n",
        "  approx = cv2.approxPolyDP(c, accuracy, True)\n",
        "  cv2.drawContours(image, [approx], 0, (0, 255, 0), 2)\n",
        "  cv2.imshow('Approx poly DP', image)\n",
        "\n",
        " cv2.waitkey(0)\n",
        " cv2.destroyAllWindows \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkIs4ETyhHD5"
      },
      "source": [
        "CONVEX HULL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCJDaVxkhJV7"
      },
      "source": [
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#threshold the image\n",
        "ret, thresh = cv2.threshold(gray, 176, 255, 0)\n",
        "\n",
        "#find contours\n",
        "contours, heirarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "#sort contours by area and then remove the largest frame contour\n",
        "n = len(contours)-1\n",
        "contours = sorted(contours, key = cv2.contourArea, reverse=False)[:n]\n",
        "\n",
        "#iterate over contours and draw a convex hull\n",
        "for c in contours:\n",
        "  hull = cv2.convexHull(c)\n",
        "  cv2.drawContours(image, [hull], 0, (0, 255, 0), 2)\n",
        "  cv2.imshow('convex Hull', image)\n",
        "\n",
        "cv2.waitkey(0)\n",
        "cv2.destroyAllWindows()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azKu9PGqTsVP"
      },
      "source": [
        "MATCHING CONTOUR SHAPES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dauv-NwHTvID"
      },
      "source": [
        "# load the shape tempelate or reference image\n",
        "tempelate = cv2.imread('images/4star.jpg', 0)\n",
        "cv2.imshow('tempelate', tempelate)\n",
        "cv2.waitkey(0)\n",
        "\n",
        "#load the target image\n",
        "target = cv2.imread('images/star.jpg')\n",
        "target_gray = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#threshhold both the images\n",
        "ret, thresh1 = cv2.threshold(tempelate, 127, 255, 0)\n",
        "ret, thresh2 = cv2.threshold(tempelate, 127, 255, 0)\n",
        "\n",
        "#finding contours\n",
        "contours, heirarchy = cv2.findContours(thresh1, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "#we need to sort the largest contour which is the shape of outline by sorting by area\n",
        "sorted_contours = sorted(contours, key = cv2.contourArea, reverse=True)\n",
        "\n",
        "#extract 2nd largest contour which will be our template contour\n",
        "template_contour = contours[1]\n",
        "\n",
        "#extract contour from second target image\n",
        "contours, heirarchy = cv2.findContours(thresh2, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "for c in contours:\n",
        "  #match the shapes\n",
        "  match = cv2.matchShapes(tempelate_contour, c, 1, 0.0)\n",
        "  print(match)\n",
        "  #if match is less than 0.15\n",
        "  if match<0.15:\n",
        "    closest_contour = c\n",
        "  else:\n",
        "    closest_contour = []\n",
        "\n",
        "cv2.drawContours(target, [closest_contour], -1, (0, 255, 0), 3)\n",
        "cv2.imshow('target', target)\n",
        "cv2.waitkey(0)\n",
        "cvw.clearAllWindows()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y-XhxqiXV1I"
      },
      "source": [
        "LINE DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xxVLgSelgG3"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "  \n",
        "# Reading the required image in \n",
        "# which operations are to be done. \n",
        "# Make sure that the image is in the same \n",
        "# directory in which this python program is\n",
        "img = cv2.imread('image.jpg')\n",
        "  \n",
        "# Convert the img to grayscale\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "# Apply edge detection method on the image\n",
        "edges = cv2.Canny(gray,50,150,apertureSize = 3)\n",
        "  \n",
        "# This returns an array of r and theta values\n",
        "lines = cv2.HoughLines(edges,1,np.pi/180, 200)\n",
        "  \n",
        "# The below for loop runs till r and theta values \n",
        "# are in the range of the 2d array\n",
        "for r,theta in lines[0]:\n",
        "      \n",
        "    # Stores the value of cos(theta) in a\n",
        "    a = np.cos(theta)\n",
        "  \n",
        "    # Stores the value of sin(theta) in b\n",
        "    b = np.sin(theta)\n",
        "      \n",
        "    # x0 stores the value rcos(theta)\n",
        "    x0 = a*r\n",
        "      \n",
        "    # y0 stores the value rsin(theta)\n",
        "    y0 = b*r\n",
        "      \n",
        "    # x1 stores the rounded off value of (rcos(theta)-1000sin(theta))\n",
        "    x1 = int(x0 + 1000*(-b))\n",
        "      \n",
        "    # y1 stores the rounded off value of (rsin(theta)+1000cos(theta))\n",
        "    y1 = int(y0 + 1000*(a))\n",
        "  \n",
        "    # x2 stores the rounded off value of (rcos(theta)+1000sin(theta))\n",
        "    x2 = int(x0 - 1000*(-b))\n",
        "      \n",
        "    # y2 stores the rounded off value of (rsin(theta)-1000cos(theta))\n",
        "    y2 = int(y0 - 1000*(a))\n",
        "      \n",
        "    # cv2.line draws a line in img from the point(x1,y1) to (x2,y2).\n",
        "    # (0,0,255) denotes the colour of the line to be \n",
        "    #drawn. In this case, it is red. \n",
        "    cv2.line(img,(x1,y1), (x2,y2), (0,0,255),2)\n",
        "      \n",
        "# All the changes made in the input image are finally\n",
        "# written on a new image houghlines.jpg\n",
        "cv2.imwrite('linesDetected.jpg', img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA8cqiUDlg4b"
      },
      "source": [
        "CIRCLE DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RUfF2FnlkJc"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import cv2.cv as cv\n",
        "\n",
        "image = cv2.imread('images/bottlecaps.jpg')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "blur = cv2.medianBlur(gray, 5)\n",
        "circles = cv2.HoughCircles(blur, cv.CV_HOUGH_GRADIENT, 1.5, 10)\n",
        "\n",
        "for i in circles[0,:]:\n",
        "       # draw the outer circle\n",
        "       cv2.circle(image,(i[0], i[1]), i[2], (255, 0, 0), 2)\n",
        "      \n",
        "       # draw the center of the circle\n",
        "       cv2.circle(image, (i[0], i[1]), 2, (0, 255, 0), 5)\n",
        "\n",
        "cv2.imshow('detected circles', image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZeW8l5qlm5D"
      },
      "source": [
        "BLOB DETECTION\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4FITgLub822"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#read image\n",
        "image = cv2.imread('images/sunflowers.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "#set up detector with default parameters\n",
        "detector = cv2.SimpleBlobDetector()\n",
        "\n",
        "#detect blobs\n",
        "keypoints = detector.detect(image)\n",
        "\n",
        "#draw detected blobs as red circles\n",
        "# cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS ensures size of circle of blob\n",
        "blank = np.zeroes((1,1))\n",
        "blobs = cv2.drawKeyPoints(image, keypoints, blank, (0, 255, 0), cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
        "\n",
        "#show keypoints\n",
        "cv2.imshow('blobs', blobs)\n",
        "cv2.waitkey(0)\n",
        "cv2,clearAllWindows()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}